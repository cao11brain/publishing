##### DNN(Deep Neural Network)
hidden layer 와 unit 에 따라서 input to output 을 계산한다 
dropout - 과적합을 방지하기 위해서 트레이닝 할 때 전체 노드가 아니라 일부를 빼서 쓰는 것 
Import 하는 법
	`import tensorflow as tf`
	`from tensorflow.keras.models import Sequential #머신러닝용 통을 만드는 것
	`from tensorflow.keras.layers import Dense, Dropout  #Dense는 히든레이어를 가져오는 것 
코드 만드는 법 
	`model = Sequential()  #깡통으로 모델을 만듬`
	`model.add(Dense(4), input_shape=(3,), activation=‘relu’)) 
		# Dense로 레이어를 추가하는데 레이어의 unit은 4개 가지고 있는 것을 한다. input 은 3개의 값을 가지는 것을 받는다. activate function 은 ’relu’ 모델을 쓴다 
	`model.add(Dropout(0.2)) 
		# 0.2 즉 20% 를 dropout 한다 
	`model.add(Dense(4, activation=‘relu’))` 
		# layer 를 하나 더 만든다 
	`model.add(Dense(1, activation=‘sigmoid’))`
		# 마지막 출력은 하나로 나오는 것으로 한다. 0 또는 1 값으로 값이 나오는 것은 sigmoid function 임
	`model.complie(loss=‘binary_crossentropy’, optimizer=‘adam’, metrics=[‘acc’])` 
		# 컴파일 시키고… loss function 으로 예측값과 실제값의 차이를 세팅 
	`history=model.fit(X_train, y_train, vaildation_data=(X_test, y_test), epochs=40, batch_size=10)`
		# epoch 40 : 40회 반복시키면서 학습을 시킨다 
		# batch size : 전체 데이터를 얼마나 나누어 학습시키느냐의 정의 
##### CNN(Convolutional Neural Network)
전체를 DNN 하기 힘들어서 특징을 뽑아내서 특징을 가지고 DNN 을 하는 방식 
==Convolutional Filter==: 특징/패턴을 어떻게 찾아내는가가 핵심임 
Padding : Feature Map 으로 인해 사이즈가 줄어든 것을 보정하기 위해 바깥에 0 을 채워주는 것 
Pooling Layer : 계산량, 메모리 사용량, 파라미터수를 줄이기 위해 축소본을 만드는 것
##### RNN(Recurrence Neural Network) 
순서가 있는 언어 같은 것에 쓰는 것 
구조도 다양하다 : one to one, one to many(예, 사진설명 붙이기), many to one(예, 감성분류)

##### 딥러닝 프로세스
1. 라이브러리 임포트(import)
2. 데이터 가져오기(loading the data)
3. 탐색적 데이터 분석(Exploratory Dta Analysis)
4. 데이터 전처리(Data PreProcessing) : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 더미특성 생성, 특성 추출(feature engineering) 등 
5. Train, Test 데이터셋 분할
6. 데이터 정규화(Normalizing the Data) 
7. 모델 개발(Creating Model)
8. 모델 성능 평가 