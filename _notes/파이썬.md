## Basic
관리 : Indexing / Slicing 
자료형 : string, integer, float
자료형 : List / Tuple / Dictionary

List 함수 : 

### 데이터 가져오는 방식
loc 
- 이름/라벨 기준으로 슬라이싱, 인덱싱 하는 경우 사용 
iloc
- 행 번호를 기준으로 슬라이싱, 인덱싱 하는 경우 사용 — ==똑같은 경우인데 숫자로 인덱싱 하는 것==
- 즉, 인덱스 값이 따로 설정되어 있더라도 베이스 인덱스 기준으로 가져옴

### 데이터프레임 변경하기 
groupby 
pivot(index, columns, values)
pivot_table(index, columns, values, aggfunc)
stack - 컬럼 레벨에서 인덱스 레벨로 바꾸는 것 — set_index
unstack - 인덱스 레벨을 컬럼 레벨로 바꾸는 것 — reset_index

### 데이터프레임 병합하기
concat(…, ignore_index=False, axis=0, …) : 데이터 속성 형태(즉, colums 명이) 동일할 경우 
- ignore_index : Fales - 각각의 인덱스값을 유지하면서 데이터를 병합 
- axis : 1 - Columns 을 추가하면서 좌우로 합침 .  0 - Row 레벨로 위아래로 데이터를 합침
- verify_integrity : False - index가 중복된 값을 무시하고 중복으로 합침, True - 중복 index가 있으면 오류 
merge(…, on=‘index’, how=‘inner’, …) - inner, left, right, outer
- on : merge 의 기준이 되는 Column 값을 지정해 줌 
- how : inner - 값이 있는 경우만 보여줌(없으면 안함) 
join  - index 를 기준으로 merge 하는 방식. index 가 없으면 set_index 로 index 를 잡아주기도 한다

### 데이터 탐색하기 
astype : 변환하고자 하는 Column 명, type 을 지정해서 변환하기. 예) `df.astype({‘age’:flate})`
replace : 특정 문자를 변경하기. 예) `df.replace(“_”, np.NAN)`

### 결측치 처리하기 
특징
- 실무자의 견해가 많이 반영
- 시간이 많이 투자됨 
- 데이터 편향이 발생 가능함(bias) - 특히, 단순대체할 경우 심해짐 
방법
- Listwise : 결측치가 하나라도 있을 경우 삭제 —> 최대치의 정보손실이 발생함
- Pairwise : 모든 데이터가 결측치일 경우 삭제 
- 대체하기 : 평균값(mean), 중앙값(medium), 최대값(mode) 등으로 채우기
- 예측하기 : 주변값과의 상관관계나 예측모델을 활용해서 채움 
함수 
- 채우기 : df.fillna(‘value’, …, method=None, …) - `method 변수 : backfill, ffill, None`
- 바꾸기 : df = df.replace(np.nan, df.meidan())
- 같은 간격으로 채우기 : df.interpolate() - liner 하게 
- 제거하기 - df.dropna(…, how=‘any’, thresh=N, subset=Column Name, …)  
	- 기본적으로 lisewise 방식을 사용함. how = ‘all’ 을 하면 pairwise 사용 
	- thresh = N , NA 값이 N 개 이상일 경우에는 삭제하지 않는다 
	- subset , 제거할 때 참조할 특정한 열이름을 지정해서 하면 된다
==데이터 처리 시, df.copy() 를 사용해서 해야함. copy() 를 안쓰면 주소 참조해서 쓰게 된다== 

### 이상치 처리하기 
이상치 확인하기 
- value_counts() 로 데이터 현황 확인 가능 
이상치 변경하기 
- 변수형 데이터의 경우, `replace(‘A’,‘B’)` 와 같은 함수를 사용할 수 있음
- 숫자형 데이터의 경우, `quantile(0.25) quantile(0.75)` 를 활용해 이상치 한계 (`1.5*(q3-q1)`) 을 초과하는 경우만 체크해서 바꿔줌 

### Feature Engineering 
Binning : 연속형 변수를 적당히 그룹핑 해서 데이터의 해석을 높이는 방법
- cut : 구간 값을 지정해서 그룹핑 하는 방법
	- `pd.cut(df, bins=[…, …, …], labels=[value, value, …])`
- qcut : 구간의 개수를 지정해서 그룹핑 하는 방법 
	- `pd.qcut(df, Numbers, labes=[vales, …])` - Numbers 는 나눌 구간 개수만 지정해주면 됨
Scaling : 숫자 데이터간의 상대적 크기 차이를 제거하는 방법(Nomalization 같은 거.. )
- standardScaler() : 평균 0, 분산 1 인 정규분포로 만듬 
- RobustScaler() : 평균, 분산 대신 - ==이상치 영향을 덜 받음==
- MinMaxScaler() : 모든 특성이 0~1 사이에 위치하도록 함. ==분류보다 회귀분석에 유용함 ==
	`from sklearn.preprocessing import MinMaxScalar
	`scaler = MinMaxScalar()
	`df[:] = scaler.fit_transform(df[:]) 
- MaxAbsScaler() : 절대값이 0~1 사이에, 즉 -1~1 사이에 위치하도록 함
One Hot Encoding : 머신러닝/딥러닝은 수치형 데이터만 가능 —> 수치형으로 변환이 필요
- 값(Value) 를 Colums 로 변환하고 Colums 별로 Value 를 0 또는 1로 바꾸어 수치형으로 바꾼다
	`pd.get_dummies(df, colums=[‘Name’])` 
	colums 를 미리 사전에 세팅해도 된다. 예를 들어 colums = [‘A’, ‘B’] 와 같이 쓰고 
	`pd.get_dummies(df, colums = colums)` 로 설정
- get_dummy 를 하면 바로 OneHotEncoding 이 된다. 기존 데이터에 붙이려면 위와 같은 문법을 사용하면 된다

## 데이터 시각화 
### Matplotlib 
##### 그리는 순서는 다음과 같이 진행됨  
1. 시각화 영역 지정 `plt.figure(…, figsize=(%f,%f),…)
2. 시각화 차트 및 값 지정 `plt.plot` 
3. ==서브플랏이 있을 경우, `plt.subplot(221)`  - 나눈 행/열 중에서 몇번째 영역에 위치하는가의 의미==
4. 출력 `plt.show()`
##### 시각화 차트 종류
`plt.plot(data, kind=…, color= , maker=‘o’, linestyle= ,…)`
- kind 파라미터를 지정해서 bar 차트를 그릴 수도 있다. 
`plt.scatter(x=df[‘Name’],y=df[‘Name’])`
`plt.hist(x, bins=10, …)` : 수치형 데이터의 분포를 나타냄. 빈도, 빈도밀도, 확률 등
- bins 로 빈도 영역의 개수를 나타냄. 기본은 10개 구간으로 나뉨 
`plt.boxplot(x, by=‘column’, column=‘colunm’, …)` 
- 5가지 수치로 그래프를 만듦(최소값, 1사분위, 2사분위, 3사분위, 최대값)
- by : x축에 대해서, column : boxplot을 그려 줄 데이터
`plt.bar(x, height, stacked=True)` : 가로, 세로, 누적, 그룹화 막대 그래프도 그릴 수 있음
- stacked 파라미터 : True 일 경우 누적막대그래프
##### 차트 꾸미기
`plt.xlable(‘Name’)
`plt.ylabel(’Name’)`
`plt.title(’Name’)`
`plt.legend([레이블1번, 레이블2번])`
`plt.grid()` - 보조선을 함께 그려주는 것
### Seaborn 
Matplotlib 패키지 기반 시각화 강화한 패키지 
##### 시각화 차트 종류 
`sns.scatterplot(x, y)` : x와 y 값을 지정해야 함 
`sns.catplot(x, y, col or row=“Name”, col_wrap=N)` 
- 색상과 열 또는 행을 사용해 3개 이상의 카테고리에 대한 분포 변화를 보여줌
- col_wrap 번호에 따라 가로로 배치함 
`sns.lmplot(x, y, … , hue=‘Name’, line_kws={‘color’:’red’, … })` 
- 산점도에 회귀선을 그려줌
- line_kws 에 회귀선의 조건을 정의해줄 수 있음
- hue 값으로 카테고리(변수)별로 색깔을 다르게 할 수 있다 
`sns.Countplot(x)` 
- 항목별 갯수를 확인할 때 사용함
- 히스토그램은 연속형 변수만 가능한데, 카운터플롯은 범주형 변수도 가능함 
- y 에 값을 넣으면 가로 막대 그래프가 나옴
`sns.boxplot(x,y)`
`sns.violinplot(x, y)`
- boxplot 하고 비슷하지만 밀집도를 함께 볼 수 있어서 밀집도를 볼 수 있음 
`sns.jointplot(x, y, …, kind =‘hex’, …)`
- 산점도와 카운터 플롯을 한번에 보여줌 
- kind=‘hex’ 로 지정할 경우, 색깔로 강도를 표시해 줌
`sns.heatmap(data)

### 머신러닝 모델링
##### 모델링
Linear Regression 
- cost function 으로 최적의 모델을 구함 
- cost function = (실제값-예측값)^2 / N = MSE(Mean Square Error) 
##### 지도학습
분류모델(Classification)
- 비연속적인 값으로 되어 있을 경우 
예측모델(Regression, 회귀모델) 
- 연속적인 값으로 되어 있을 경우
Confusion Matrix 
- TP(True Positive) : True 를 True 로 예측함 (앞에는 맞췄냐 틀렸냐, 뒤는 어떤 값으로 예측했냐)
- FN(False Negative) : True 를 False 로 예측함 
성능지표 
- 정밀도(Precision) = TP / (TP+FP) , True 라고 예측한 것 중에서 맞춘 것의 갯수 
- 재현율(Recall) = TP / (TP+FN) , 실제 True 인 것 중에서 모델에서도 True 로 예측한 비율
- 정확도(Accuracy)
- F1-score
##### 알고리즘
머신러닝 패키지 : scikit-learn 패키지에 대부분 내장 
##### Linear Regression 
`from sklearn.family import Model` 의 형태로.. 예를 들면
	`from sklearn.linear_model import LinearRegression`
	`model=LinearRegression()`
==모델링 방법.. 아주 쉽다 - 4단계면 끝난다. 모델 불러오고 저장한뒤 학습, 검증 순서==
	`from sklearn.linear_model import LinearRegression`
	`model=LinearRegression()`
	`model.fit(X_train, y_train)
	`pred=model.predict(X_test)`
##### K-Nearest Neighbor
코딩 예시
	`from sklearn.neighbors import KNeiborsClassifier
	`knn = KNeighborsClassifier(n+neighbors=3)`
	`knn.fit(X_train, y_train)`
	`pred = knn.predict(X_test)
##### Random Forest (앙상블 기법, 여러개 모델을 섞어서)
코딩 예시
	`from sklearn.ensemble import RandomForestClassifire`
	`model=RandomForestClassifier(n_estimators=50)`
	`model.fit(X_train, y_train)`
	`pred=model.predict(X_test)`
적절한 하이퍼 파라미터 튜닝이 중요함 
##### 앙상블 기법이란?? 
###### Boosting 
순차적 학습을 하면서 잘못된 예측에 weight 를 부여해서 오차를 보완해나감
순차적 학습 = Sequantial (<=> parallel 방식, RandomForest)
모델 : XGBoost, LightGBM 
	`!pip install xgboost
	`from xgboost import XGBClassifier
	`model=XGBClassifier(n_estimator=50)
	`model=fit(X_train, y_train)
	`pred=model.predict(X_test)
###### Bagging 
###### Stacking 
여러개 모델이 예측한 결과 데이터를 가지고 다시 final_estimator 모델로 종합해 예측함
	`from sklearn.ensemble import StackingRegressor, StackingClassifier
	`stack_model = [ (‘LogisticRegression’, lg), …, ]
	`stacking = StackingClassifier(stack_models, final_estimator=rft, n_jobs=1)`
- final_estimator 로 마지막으로 예측을 하게 됨 
###### Weighted Blending 
모델의 예측값에 대해 weigh 를 곱해서 최종 output 을 계산함 

##### 데이터셋 분할 
`from sklearn.model_selection import train_test_split`
`x = df1.drop(‘termination_Y’, axis=1).values
`y = df1[‘termination_Y’].values
`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, rando_state=42)`
- stratify = ‘Name’ 어떤 데이터 값에 대해서 동일한 비율로 나누라는 뜻 
###### 데이터 정규화(Normaizing/Scailing) 
`from sklearn.preprocessing import MinMaxScaler
`scaler=MinMaxScaler() 
`X_train = scaler.fit_transform(X_train)
`X_test = scaler.transform(X_test)


### 딥러닝
##### DNN(Deep Neural Network)
hidden layer 와 unit 에 따라서 input to output 을 계산한다 
dropout - 과적합을 방지하기 위해서 트레이닝 할 때 전체 노드가 아니라 일부를 빼서 쓰는 것 
Import 하는 법
	`import tensorflow as tf`
	`from tensorflow.keras.models import Sequential #머신러닝용 통을 만드는 것
	`from tensorflow.keras.layers import Dense, Dropout  #Dense는 히든레이어를 가져오는 것 
코드 만드는 법 
	`model = Sequential()  #깡통으로 모델을 만듬`
	`model.add(Dense(4), input_shape=(3,), activation=‘relu’)) 
		# Dense로 레이어를 추가하는데 레이어의 unit은 4개 가지고 있는 것을 한다. input 은 3개의 값을 가지는 것을 받는다. activate function 은 ’relu’ 모델을 쓴다 
	`model.add(Dropout(0.2)) 
		# 0.2 즉 20% 를 dropout 한다 
	`model.add(Dense(4, activation=‘relu’))` 
		# layer 를 하나 더 만든다 
	`model.add(Dense(1, activation=‘sigmoid’))`
		# 마지막 출력은 하나로 나오는 것으로 한다. 0 또는 1 값으로 값이 나오는 것은 sigmoid function 임
	`model.complie(loss=‘binary_crossentropy’, optimizer=‘adam’, metrics=[‘acc’])` 
		# 컴파일 시키고… loss function 으로 예측값과 실제값의 차이를 세팅 
	`history=model.fit(X_train, y_train, vaildation_data=(X_test, y_test), epochs=40, batch_size=10)`
		# epoch 40 : 40회 반복시키면서 학습을 시킨다 
		# batch size : 전체 데이터를 얼마나 나누어 학습시키느냐의 정의 
##### CNN(Convolutional Neural Network)
전체를 DNN 하기 힘들어서 특징을 뽑아내서 특징을 가지고 DNN 을 하는 방식 
==Convolutional Filter==: 특징/패턴을 어떻게 찾아내는가가 핵심임 
Padding : Feature Map 으로 인해 사이즈가 줄어든 것을 보정하기 위해 바깥에 0 을 채워주는 것 
Pooling Layer : 계산량, 메모리 사용량, 파라미터수를 줄이기 위해 축소본을 만드는 것
##### RNN(Recurrence Neural Network) 
순서가 있는 언어 같은 것에 쓰는 것 
구조도 다양하다 : one to one, one to many(예, 사진설명 붙이기), many to one(예, 감성분류)

##### 딥러닝 프로세스
1. 라이브러리 임포트(import)
2. 데이터 가져오기(loading the data)
3. 탐색적 데이터 분석(Exploratory Dta Analysis)
4. 데이터 전처리(Data PreProcessing) : 데이터타입 변환, Null 데이터 처리, 누락데이터 처리, 더미특성 생성, 특성 추출(feature engineering) 등 
5. Train, Test 데이터셋 분할
6. 데이터 정규화(Normalizing the Data) 
7. 모델 개발(Creating Model)
8. 모델 성능 평가 





